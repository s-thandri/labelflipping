{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s-thandri/labelflipping/blob/main/acs_income.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Import the libraries and build the parity and LR function <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hqkuSkbjIru1"
      },
      "outputs": [],
      "source": [
        "#Import all of the necessary libraries in\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from scipy.io import arff\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This function is being used to calculate the statistical parity of the data set\n",
        "#test_data: The original data frame containing the test data\n",
        "#predictors: List of the all the column names AKA IV's\n",
        "#prediction_df: Contains the predicitons made by the DT model\n",
        "#sensitive_attr: Name of the sensitive attr\n",
        "#sensitive_attr_cutoff: Cut off value for sensitive attr\n",
        "#concat_col: Name of column I am creating new dataframe on\n",
        "\n",
        "def s_parity(test_data, predictors, prediction_df, sensitive_attr, concat_col):\n",
        "    #Creating a new DF that contains all the datapoints from the test data and the predictions made from LR model\n",
        "    #Concat_col: outcome\n",
        "    test_demo_df = pd.DataFrame(test_data, columns = predictors)\n",
        "    predicted_df = pd.DataFrame(prediction_df, columns = [concat_col])\n",
        "    concat_df = pd.concat([test_demo_df,predicted_df], axis=1)\n",
        "\n",
        "    #Get the two groups of people totals\n",
        "    total_unpriv = (len(concat_df[concat_df[sensitive_attr]==0]))\n",
        "    total_priv = (len(concat_df[concat_df[sensitive_attr]==1]))\n",
        "\n",
        "    #Number of people accepted\n",
        "    total_credit_unpriv = len(concat_df[(concat_df[concat_col] == 1) & (concat_df[sensitive_attr] == 0)])\n",
        "    total_credit_priv = len(concat_df[(concat_df[concat_col] == 1) & (concat_df[sensitive_attr] == 1)])\n",
        "\n",
        "    #Percentage of approved people\n",
        "    p_unpriv = total_credit_unpriv/total_unpriv\n",
        "    p_priv = total_credit_priv/total_priv\n",
        "\n",
        "\n",
        "    #Calculate the parity\n",
        "    parity = p_priv - p_unpriv\n",
        "\n",
        "\n",
        "    return parity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Make changes to make more efficient\n",
        "#Function used to run the DT model\n",
        "#train_dataset: Training dataset to train the model\n",
        "#independent_var: Column names\n",
        "#dependent_var: Prediction column name\n",
        "#concat_col: Name of column creating new DF on\n",
        "def logistic_regression(train_dataset, independent_var, dependent_var, concat_col):\n",
        "        #Split the data up into train and test values and then run the DT model\n",
        "        #These steps aren't neccessary to consistenly do over and over again\n",
        "        x = train_dataset[independent_var].values\n",
        "        y = train_dataset[dependent_var].values\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state=310)\n",
        "        clf = LogisticRegression(class_weight=None, max_iter=100)\n",
        "        log = clf.fit(x_train,y_train)\n",
        "        prediction = log.predict(x_test)\n",
        "\n",
        "        #Printing the Overall accuracy of the model after one run\n",
        "        #F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
        "        #file.write(f'\\nAccuracy of the model on Testing Sample Data: {F1_Score}')\n",
        "\n",
        "        #Prints out the average across all ten run throughs\n",
        "        #Accuracy_Values=cross_val_score(log, x , y, cv=10, scoring='f1_weighted')\n",
        "\n",
        "        accuracy = accuracy_score(y_test,prediction)*100\n",
        "\n",
        "        #After running the model I return a df with the datapoints and the labels\n",
        "        test_demo_df = pd.DataFrame(x_test, columns = independent_var)\n",
        "        #Concat_col: credit_risk_12\n",
        "        predicted_df = pd.DataFrame(prediction, columns = [concat_col])\n",
        "\n",
        "        return accuracy, test_demo_df, predicted_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Load the data into the dataframe <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load all the data into the acs_dataframe\n",
        "acs_dataframe = pd.read_csv('acs_data/acs_income.csv', index_col=None, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGEP</th>\n",
              "      <th>COW</th>\n",
              "      <th>SCHL</th>\n",
              "      <th>MAR</th>\n",
              "      <th>OCCP</th>\n",
              "      <th>POBP</th>\n",
              "      <th>RELP</th>\n",
              "      <th>WKHP</th>\n",
              "      <th>SEX</th>\n",
              "      <th>RAC1P</th>\n",
              "      <th>ST</th>\n",
              "      <th>PINCP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4720.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3605.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7330.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>24000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2722.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>180.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3870.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>29000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664495</th>\n",
              "      <td>39.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6260.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>9600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664496</th>\n",
              "      <td>38.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4251.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>2400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664497</th>\n",
              "      <td>37.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7750.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>19700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664498</th>\n",
              "      <td>47.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8990.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>18700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1664499</th>\n",
              "      <td>34.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6260.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>7900.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1664500 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         AGEP  COW  SCHL  MAR    OCCP  POBP  RELP  WKHP  SEX  RAC1P    ST  \\\n",
              "0        18.0  1.0  18.0  5.0  4720.0  13.0  17.0  21.0  2.0    2.0   1.0   \n",
              "1        53.0  5.0  17.0  5.0  3605.0  18.0  16.0  40.0  1.0    1.0   1.0   \n",
              "2        41.0  1.0  16.0  5.0  7330.0   1.0  17.0  40.0  1.0    1.0   1.0   \n",
              "3        18.0  6.0  18.0  5.0  2722.0   1.0  17.0   2.0  2.0    1.0   1.0   \n",
              "4        21.0  5.0  19.0  5.0  3870.0  12.0  17.0  50.0  1.0    1.0   1.0   \n",
              "...       ...  ...   ...  ...     ...   ...   ...   ...  ...    ...   ...   \n",
              "1664495  39.0  6.0  16.0  5.0  6260.0  72.0   0.0  20.0  1.0    1.0  72.0   \n",
              "1664496  38.0  6.0  14.0  5.0  4251.0  72.0   0.0  32.0  1.0    8.0  72.0   \n",
              "1664497  37.0  1.0  19.0  3.0  7750.0  17.0  13.0  40.0  2.0    9.0  72.0   \n",
              "1664498  47.0  1.0  16.0  1.0  8990.0  72.0   1.0  40.0  1.0    8.0  72.0   \n",
              "1664499  34.0  1.0  16.0  5.0  6260.0  72.0   2.0  40.0  1.0    1.0  72.0   \n",
              "\n",
              "           PINCP  \n",
              "0         1600.0  \n",
              "1        10000.0  \n",
              "2        24000.0  \n",
              "3          180.0  \n",
              "4        29000.0  \n",
              "...          ...  \n",
              "1664495   9600.0  \n",
              "1664496   2400.0  \n",
              "1664497  19700.0  \n",
              "1664498  18700.0  \n",
              "1664499   7900.0  \n",
              "\n",
              "[1664500 rows x 12 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acs_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> Conducting basic information gathering on a dataset <h3>\n",
        "<h4> Important information about the dataset regarding columns <h4>\n",
        "<ul>\n",
        "<li>AGEP: Age</li>\n",
        "<li>COW: Class of Worker</li>\n",
        "<li>SCHL: Educational Attainment</li>\n",
        "<li>MAR: Marital Status</li>\n",
        "<li>OCCP: Occupation</li>\n",
        "<li>POBP: Place of Birth</li>\n",
        "<li>RELP: Relationship to Householders</li>\n",
        "<li>WKHP: Usual Hours worked per week</li>\n",
        "<li>SEX: Sex</li>\n",
        "<li>RAC1P: Race</li>\n",
        "<li>ST: State Codes</li>\n",
        "<li>PINCP: Total Annual Income</li>\n",
        "</ul>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Fixing the noation \n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)  # Set decimal precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGEP</th>\n",
              "      <th>COW</th>\n",
              "      <th>SCHL</th>\n",
              "      <th>MAR</th>\n",
              "      <th>OCCP</th>\n",
              "      <th>POBP</th>\n",
              "      <th>RELP</th>\n",
              "      <th>WKHP</th>\n",
              "      <th>SEX</th>\n",
              "      <th>RAC1P</th>\n",
              "      <th>ST</th>\n",
              "      <th>PINCP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "      <td>1664500.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>43.41</td>\n",
              "      <td>2.08</td>\n",
              "      <td>18.62</td>\n",
              "      <td>2.52</td>\n",
              "      <td>4180.52</td>\n",
              "      <td>65.82</td>\n",
              "      <td>2.24</td>\n",
              "      <td>38.33</td>\n",
              "      <td>1.48</td>\n",
              "      <td>1.87</td>\n",
              "      <td>28.13</td>\n",
              "      <td>56663.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>15.30</td>\n",
              "      <td>1.83</td>\n",
              "      <td>3.30</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2658.72</td>\n",
              "      <td>93.06</td>\n",
              "      <td>4.39</td>\n",
              "      <td>13.08</td>\n",
              "      <td>0.50</td>\n",
              "      <td>2.08</td>\n",
              "      <td>16.32</td>\n",
              "      <td>73067.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>17.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>104.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>30.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>16.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2205.00</td>\n",
              "      <td>18.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>35.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>12.00</td>\n",
              "      <td>20000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>43.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>19.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4200.00</td>\n",
              "      <td>36.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>40.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>28.00</td>\n",
              "      <td>39000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>56.00</td>\n",
              "      <td>3.00</td>\n",
              "      <td>21.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>5740.00</td>\n",
              "      <td>48.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>44.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>42.00</td>\n",
              "      <td>68000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>96.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>24.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>9830.00</td>\n",
              "      <td>554.00</td>\n",
              "      <td>17.00</td>\n",
              "      <td>99.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>72.00</td>\n",
              "      <td>1423000.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            AGEP        COW       SCHL        MAR       OCCP       POBP  \\\n",
              "count 1664500.00 1664500.00 1664500.00 1664500.00 1664500.00 1664500.00   \n",
              "mean       43.41       2.08      18.62       2.52    4180.52      65.82   \n",
              "std        15.30       1.83       3.30       1.80    2658.72      93.06   \n",
              "min        17.00       1.00       1.00       1.00      10.00       1.00   \n",
              "25%        30.00       1.00      16.00       1.00    2205.00      18.00   \n",
              "50%        43.00       1.00      19.00       1.00    4200.00      36.00   \n",
              "75%        56.00       3.00      21.00       5.00    5740.00      48.00   \n",
              "max        96.00       8.00      24.00       5.00    9830.00     554.00   \n",
              "\n",
              "            RELP       WKHP        SEX      RAC1P         ST      PINCP  \n",
              "count 1664500.00 1664500.00 1664500.00 1664500.00 1664500.00 1664500.00  \n",
              "mean        2.24      38.33       1.48       1.87      28.13   56663.86  \n",
              "std         4.39      13.08       0.50       2.08      16.32   73067.45  \n",
              "min         0.00       1.00       1.00       1.00       1.00     104.00  \n",
              "25%         0.00      35.00       1.00       1.00      12.00   20000.00  \n",
              "50%         1.00      40.00       1.00       1.00      28.00   39000.00  \n",
              "75%         2.00      44.00       2.00       1.00      42.00   68000.00  \n",
              "max        17.00      99.00       2.00       9.00      72.00 1423000.00  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Describe the dataset\n",
        "acs_dataframe.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5704"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Understand the spread of the OCCUPATIONS\n",
        "#0010-0440\t11-0000\tManagement Occupations: 170997\n",
        "#0500-0960\t13-0000\tBusiness and Financial Operations Occupations: 91842\n",
        "#1005-1240\t15-0000\tComputer and mathematical occupations: 50817\n",
        "#1305-1560\t17-0000\tArchitecture and Engineering Occupations: 31718\n",
        "#1600-1980\t19-0000\tLife, Physical, and Social Science Occupations: 16529\n",
        "#2001-2970\t21-0000 - 27-0000\tEducation, Legal, Community Service, Arts, and Media Occupations: 193762\n",
        "#3000-3550\t29-0000\tHealthcare Practitioners and Technical Occupations: 100986\n",
        "#3601-4655\t31-0000 - 39-0000\tService Occupations: 283912\n",
        "#4700-5940\t41-0000 - 43-0000\tSales and Office Occupations: 358340\n",
        "#6005-7640\t45-0000 - 49-0000\tNatural Resources, Construction, and Maintenance Occupations: 143613\n",
        "#7700-9760\t51-0000 - 53-0000\tProduction, Transportation, and Material Moving Occupations: 216280\n",
        "#9800-9920                      Military Specific Occupations: 5704\n",
        "((acs_dataframe['OCCP'] >= 9800) & (acs_dataframe['OCCP'] <= 9920)).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Level of Education Grouped<h4>\n",
        "<ul>\n",
        "<li>Grade School (No Diploma): 109,882</li>\n",
        "<li>High School Diploma/GED: 400,706 </li>\n",
        "<li>Some College (2 Years at Most): 531,044 </li>\n",
        "<li>Bachelor's Degree: 366,380</li>\n",
        "<li>Master's Degree: 160,594 </li>\n",
        "<li>Professional Degree: 41,426</li>\n",
        "<li>Doctorate Or Equivalent: 27530</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Group Info in columns to ranges so there is less unique values for making Dummies <h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Group the SCHL columns into a single values\n",
        "#1: Grade School (No Diploman)\n",
        "#2: High School Diploma/GED\n",
        "#3: Some College (2 Years at Most)\n",
        "#4: Bachelor's Degree\n",
        "#5: Master's Degree\n",
        "#6: Professional Degree\n",
        "#7: Doctorate Or Equivalent\n",
        "ranges = {'GS': (1, 15), 'HSD': (16, 17), 'SC': (18, 20), 'BD': (21,21), 'ME': (22,22), 'PD': (23,23), 'DE': (24,24)}\n",
        "group_dict = {}\n",
        "\n",
        "def assign_group(value):\n",
        "  \"\"\"Assigns a group label to a value based on the ranges dictionary.\"\"\"\n",
        "  if value not in group_dict:\n",
        "    for group_name, group_range in ranges.items():\n",
        "      if group_range[0] <= value <= group_range[1]:\n",
        "        group_dict[value] = group_name\n",
        "        break\n",
        "  return group_dict.get(value)\n",
        "\n",
        "acs_dataframe['SCHL'] = acs_dataframe['SCHL'].apply(assign_group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Assign codes to occupations based on ranges\n",
        "#0010-0440\t11-0000\tManagement Occupations: 170997\n",
        "#0500-0960\t13-0000\tBusiness and Financial Operations Occupations: 91842\n",
        "#1005-1240\t15-0000\tComputer and mathematical occupations: 50817\n",
        "#1305-1560\t17-0000\tArchitecture and Engineering Occupations: 31718\n",
        "#1600-1980\t19-0000\tLife, Physical, and Social Science Occupations: 16529\n",
        "#2001-2970\t21-0000 - 27-0000\tEducation, Legal, Community Service, Arts, and Media Occupations: 193762\n",
        "#3000-3550\t29-0000\tHealthcare Practitioners and Technical Occupations: 100986\n",
        "#3601-4655\t31-0000 - 39-0000\tService Occupations: 283912\n",
        "#4700-5940\t41-0000 - 43-0000\tSales and Office Occupations: 358340\n",
        "#6005-7640\t45-0000 - 49-0000\tNatural Resources, Construction, and Maintenance Occupations: 143613\n",
        "#7700-9760\t51-0000 - 53-0000\tProduction, Transportation, and Material Moving Occupations: 216280\n",
        "#9800-9920                      Military Specific Occupations: 5704\n",
        "\n",
        "ranges = {\n",
        "    'management': (10,440),\n",
        "    'business': (500,960),\n",
        "    'computer': (1005,1240),\n",
        "    'engineering': (1305,1560),\n",
        "    'life': (1600,1980),\n",
        "    'education_arts': (2001,2970),\n",
        "    'healthcare': (3000,3550),\n",
        "    'service': (3601,4655),\n",
        "    'sales': (4700,5940),\n",
        "    'environmental_construction': (6005,7640),\n",
        "    'production_transportation': (7700,9760),\n",
        "    'military': (9800,9920)\n",
        "}\n",
        "group_dict = {}\n",
        "acs_dataframe['OCCP'] = acs_dataframe['OCCP'].apply(assign_group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Dont group the states because there is no reason\n",
        "#Ask if I should group it but probably wont need to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Add family codes \n",
        "#1-7: Family\n",
        "#8-10: inlaws and other family\n",
        "#11-17: Non-Family\n",
        "ranges = {\n",
        "    'family': (0,7),\n",
        "    'inlaws/other': (8,10),\n",
        "    'non-family': (11,17)\n",
        "}\n",
        "group_dict = {}\n",
        "acs_dataframe['RELP'] = acs_dataframe['RELP'].apply(assign_group)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Code the Class of worker column from numeric to strings\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(1, 'private_business')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(2, 'non-profit')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(3, 'local_gov')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(4, 'state_gov')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(5, 'federal_gov')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(6, 'SE_no_business')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(7, 'SE_business')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(8, 'no_pay_work')\n",
        "acs_dataframe['COW'] = acs_dataframe['COW'].replace(9, 'unemployed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Class the Marital Status from numeric to strings\n",
        "acs_dataframe['MAR'] = acs_dataframe['MAR'].replace(1, 'married')\n",
        "acs_dataframe['MAR'] = acs_dataframe['MAR'].replace(2, 'widowed')\n",
        "acs_dataframe['MAR'] = acs_dataframe['MAR'].replace(3, 'divorced')\n",
        "acs_dataframe['MAR'] = acs_dataframe['MAR'].replace(4, 'seperated')\n",
        "acs_dataframe['MAR'] = acs_dataframe['MAR'].replace(5, 'never_married')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Code the sex column from numeric to string\n",
        "acs_dataframe['SEX'] = acs_dataframe['SEX'].replace(1, 'male')\n",
        "acs_dataframe['SEX'] = acs_dataframe['SEX'].replace(2, 'female')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Code the race column from numeric to string\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(1, 'white')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(2, 'black')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(3, 'american_indian')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(4, 'alaska_native')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(5, 'native_american')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(6, 'asian')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(7, 'native_hawaiian')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(8, 'other')\n",
        "acs_dataframe['RAC1P'] = acs_dataframe['RAC1P'].replace(9, 'mixed_race')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Code the PINCP column so that it is binary\n",
        "ranges = {\n",
        "    0: (0,80000),\n",
        "    1: (80001,2000000)\n",
        "}\n",
        "group_dict = {}\n",
        "acs_dataframe['PINCP'] = acs_dataframe['PINCP'].apply(assign_group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Build the Model to check accuracy and original parity<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Encode the non-numeric columns\n",
        "columns_to_encode = ['AGEP','COW','SCHL','MAR','OCCP','POBP','RELP','SEX','RAC1P']\n",
        "encoded_data = pd.get_dummies(acs_dataframe, columns=columns_to_encode, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Get the columns for the predictors and target variable\n",
        "acs_columns_list = list(encoded_data.columns)\n",
        "\n",
        "predictors = [item for item in acs_columns_list if item != 'PINCP']\n",
        "target_var = 'PINCP'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Setting the columns to each part of the logistic regression\n",
        "#x conatins the IV's\n",
        "#y contains the DV\n",
        "x = encoded_data[predictors].values\n",
        "y = encoded_data[target_var].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=310)\n",
        "#X-train is all the data points for training\n",
        "#y_train contains the labels for each of the training data points\n",
        "#x_test contains all the testing data points\n",
        "#y_test contains the ground truth for each of the test data points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "85.9954941423851\n"
          ]
        }
      ],
      "source": [
        "#Run the LR classifier\n",
        "clf = LogisticRegression(class_weight=None, max_iter=100)\n",
        "lr = clf.fit(x_train,y_train)\n",
        "prediction = lr.predict(x_test)\n",
        "prediction_prob = lr.predict_proba(x_train)\n",
        "accuracy = accuracy_score(y_test,prediction)*100\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistical Parity: 0.13554\n"
          ]
        }
      ],
      "source": [
        "#Break the test dataframe into different groups\n",
        "#Unprotected group is everyone who is a female or 2\n",
        "#Complement group is everyone who is a male or 1\n",
        "test_demo_df = pd.DataFrame(x_test, columns = predictors)\n",
        "predicted_df = pd.DataFrame(prediction, columns = ['income'])\n",
        "concat_df = pd.concat([test_demo_df,predicted_df], axis=1)\n",
        "\n",
        "#Get the two groups of people totals\n",
        "total_unpriv = (len(concat_df[concat_df['SEX_male']==0]))\n",
        "total_priv = (len(concat_df[concat_df['SEX_male']==1]))\n",
        "\n",
        "#Number of people accepted\n",
        "total_credit_unpriv = len(concat_df[(concat_df['income'] == 1) & (concat_df['SEX_male'] == 0)])\n",
        "total_credit_priv = len(concat_df[(concat_df['income'] == 1) & (concat_df['SEX_male'] == 1)])\n",
        "\n",
        "#Percentage of approved people\n",
        "p_unpriv = total_credit_unpriv/total_unpriv\n",
        "p_priv = total_credit_priv/total_priv\n",
        "\n",
        "statistical_parity = p_priv - p_unpriv\n",
        "print(f'Statistical Parity: {statistical_parity:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Implement the Different Solutions now<h3>\n",
        "<ul>\n",
        "<li>Iterative Flipping</li>\n",
        "<li>Uncertainty Reduction</li>\n",
        "<li>Model Uncertainty</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Need to build a new dataframe for training and testing purposes for iterative flipping \n",
        "#Combine x_train and y_train\n",
        "#I combine these so that when I flip the labels I have one DF that I work with\n",
        "#Rather than 2 numpy.ndarrays\n",
        "train_demo_df = pd.DataFrame(x_train, columns = predictors)\n",
        "train_outcome_df = pd.DataFrame(y_train, columns = ['PINCP'])\n",
        "train_full_df = pd.concat([train_demo_df, train_outcome_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WKHP</th>\n",
              "      <th>ST</th>\n",
              "      <th>AGEP_18.0</th>\n",
              "      <th>AGEP_19.0</th>\n",
              "      <th>AGEP_20.0</th>\n",
              "      <th>AGEP_21.0</th>\n",
              "      <th>AGEP_22.0</th>\n",
              "      <th>AGEP_23.0</th>\n",
              "      <th>AGEP_24.0</th>\n",
              "      <th>AGEP_25.0</th>\n",
              "      <th>...</th>\n",
              "      <th>SEX_male</th>\n",
              "      <th>RAC1P_american_indian</th>\n",
              "      <th>RAC1P_asian</th>\n",
              "      <th>RAC1P_black</th>\n",
              "      <th>RAC1P_mixed_race</th>\n",
              "      <th>RAC1P_native_american</th>\n",
              "      <th>RAC1P_native_hawaiian</th>\n",
              "      <th>RAC1P_other</th>\n",
              "      <th>RAC1P_white</th>\n",
              "      <th>PINCP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>40.00</td>\n",
              "      <td>12.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.00</td>\n",
              "      <td>40.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>53.00</td>\n",
              "      <td>34.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45.00</td>\n",
              "      <td>30.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40.00</td>\n",
              "      <td>30.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331595</th>\n",
              "      <td>44.00</td>\n",
              "      <td>21.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331596</th>\n",
              "      <td>40.00</td>\n",
              "      <td>48.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331597</th>\n",
              "      <td>40.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331598</th>\n",
              "      <td>25.00</td>\n",
              "      <td>25.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331599</th>\n",
              "      <td>25.00</td>\n",
              "      <td>24.00</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1331600 rows × 344 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         WKHP    ST AGEP_18.0 AGEP_19.0 AGEP_20.0 AGEP_21.0 AGEP_22.0  \\\n",
              "0       40.00 12.00     False     False     False     False     False   \n",
              "1       40.00 40.00     False     False     False     False     False   \n",
              "2       53.00 34.00     False     False     False     False     False   \n",
              "3       45.00 30.00     False     False     False     False     False   \n",
              "4       40.00 30.00     False     False     False     False     False   \n",
              "...       ...   ...       ...       ...       ...       ...       ...   \n",
              "1331595 44.00 21.00     False     False     False     False     False   \n",
              "1331596 40.00 48.00     False     False     False     False     False   \n",
              "1331597 40.00 15.00     False     False     False     False     False   \n",
              "1331598 25.00 25.00     False     False     False     False     False   \n",
              "1331599 25.00 24.00     False     False     False     False     False   \n",
              "\n",
              "        AGEP_23.0 AGEP_24.0 AGEP_25.0  ... SEX_male RAC1P_american_indian  \\\n",
              "0           False     False     False  ...    False                 False   \n",
              "1           False     False     False  ...     True                 False   \n",
              "2           False     False     False  ...     True                 False   \n",
              "3           False     False     False  ...    False                 False   \n",
              "4           False     False     False  ...    False                 False   \n",
              "...           ...       ...       ...  ...      ...                   ...   \n",
              "1331595     False     False     False  ...    False                 False   \n",
              "1331596     False     False     False  ...     True                 False   \n",
              "1331597     False     False     False  ...    False                 False   \n",
              "1331598     False     False     False  ...     True                 False   \n",
              "1331599     False     False     False  ...     True                 False   \n",
              "\n",
              "        RAC1P_asian RAC1P_black RAC1P_mixed_race RAC1P_native_american  \\\n",
              "0             False       False            False                 False   \n",
              "1             False       False            False                 False   \n",
              "2              True       False            False                 False   \n",
              "3             False       False            False                 False   \n",
              "4             False       False            False                 False   \n",
              "...             ...         ...              ...                   ...   \n",
              "1331595       False       False            False                 False   \n",
              "1331596       False       False            False                 False   \n",
              "1331597       False       False            False                 False   \n",
              "1331598       False       False            False                 False   \n",
              "1331599       False       False            False                 False   \n",
              "\n",
              "        RAC1P_native_hawaiian RAC1P_other RAC1P_white PINCP  \n",
              "0                       False       False        True     0  \n",
              "1                       False        True       False     0  \n",
              "2                       False       False       False     1  \n",
              "3                       False       False        True     1  \n",
              "4                       False       False        True     0  \n",
              "...                       ...         ...         ...   ...  \n",
              "1331595                 False       False        True     0  \n",
              "1331596                 False       False        True     0  \n",
              "1331597                  True       False       False     0  \n",
              "1331598                 False       False        True     0  \n",
              "1331599                 False       False        True     0  \n",
              "\n",
              "[1331600 rows x 344 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_full_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#When flipping the label make sure convert the flip before into its original value\n",
        "\n",
        "#Combine x_train and y_train\n",
        "#Retrain the model after combining them and then flip and calculate\n",
        "#Implement label flipping and recalculate\n",
        "\n",
        "#Each list holds a different value\n",
        "\n",
        "#list_parity holds each parity value after each flip and recalculation\n",
        "list_parity = []\n",
        "#list_acc holds the accuracy of each iteration after a flip\n",
        "list_acc = []\n",
        "#list_flip holds the row number of the flip; starts at 0 and goes through the whole\n",
        "list_flip = []\n",
        "\n",
        "#Iterating through the training dataset\n",
        "for index, row in train_full_df.iterrows():\n",
        "  #If the row that I am on has the label 1 (credit_risk is the label name) then I will flip it\n",
        "  #And then run the logistic_regression function to get the accuracy, the DF that contains datapoints,\n",
        "  #And the DF that contains the predictions\n",
        "  #The two DF's are then combined in the s_parity function to calculate parity\n",
        "    if row['PINCP'] == 1:\n",
        "      #Flip the label\n",
        "        train_full_df.at[index, 'PINCP'] = 0\n",
        "\n",
        "        #Run the logistic regression function\n",
        "        #train_full_df: training dataset\n",
        "        #predictors: IV's\n",
        "        #target_var: DV's\n",
        "        #'credit_risk12': this is the column that the labels for the test_prediction is in\n",
        "        accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df, predictors, target_var, 'income')\n",
        "\n",
        "        #list_flip: contains the row number that has just been flipped\n",
        "        #list_num: contains the accuracy value just calculated\n",
        "        list_flip.append(index)\n",
        "        list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        #After calculating the accuracy parity calculation is next\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "        #list_parity: contains the parity value after the flip\n",
        "        #print(parity)\n",
        "        list_parity.append(parity)\n",
        "\n",
        "        #Flips the label back to its original value\n",
        "        train_full_df.at[index,'PINCP'] = 1\n",
        "\n",
        "    #Repeats all the same steps as above but this time if the original label is 2\n",
        "    elif row['PINCP'] == 0:\n",
        "        train_full_df.at[index, 'PINCP'] = 1\n",
        "\n",
        "        accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df, predictors, target_var, 'income')\n",
        "        list_flip.append(index)\n",
        "        list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "        #print(parity)\n",
        "        list_parity.append(parity)\n",
        "\n",
        "        train_full_df.at[index,'PINCP'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#parity_difference: Contains the difference between flipped and original value\n",
        "parity_difference = []\n",
        "#Contains parity values that fall between .03 and -.03\n",
        "large_influence = []\n",
        "#Contains the row number of each flip\n",
        "li_row = []\n",
        "\n",
        "#Takes each of the parity values after flipping and compares it to the original parity value\n",
        "#Appends the difference to a new list\n",
        "for value in list_parity:\n",
        "    difference = abs(value) - abs(statistical_parity)\n",
        "    parity_difference.append(difference)\n",
        "\n",
        "\n",
        "#Rows are considered to be high influence if they fall in between\n",
        "for index, item in enumerate(parity_difference):\n",
        "  if statistical_parity > 0:\n",
        "    if item <= statistical_parity or item >= -1*(statistical_parity):\n",
        "        large_influence.append(item)\n",
        "        li_row.append(index)\n",
        "  else:\n",
        "    if item >= statistical_parity or item <= -1*(statistical_parity):\n",
        "        large_influence.append(item)\n",
        "        li_row.append(index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine lists into tuples and zip them\n",
        "combined_data = list(zip(large_influence, li_row))\n",
        "\n",
        "# Sort based on statistical parity values in descending order\n",
        "combined_data.sort(key=lambda x: abs(x[0]), reverse=True)\n",
        "\n",
        "# Extract sorted indices list and the sorted values\n",
        "sorted_indices_list = [index for _, index in combined_data]\n",
        "sorted_values = sorted(large_influence, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#After ranking the values it is time to now go back through the flipping process but flip the top 500 labels based on their ranking\n",
        "#Parity values after each flip\n",
        "ranked_parity = []\n",
        "#Accuracy after every flip\n",
        "ranked_acc = []\n",
        "#Index/row of the flip\n",
        "ranked_flip = []\n",
        "\n",
        "row_num = 0\n",
        "#sorted_indices is a tuple so it's coverted to a list\n",
        "ranked_indices_list = list(sorted_indices_list)\n",
        "\n",
        "#Iterate through the ranked indices and start flipping labels based on their position in the list\n",
        "for row in range(len(ranked_indices_list)):\n",
        "  #row_num: contains the row number with respect to the dataframe\n",
        "  row_num = ranked_indices_list[row]\n",
        "  #If the label at the specific row is 1 it flips it to 2 then calculates the parity and accuracy\n",
        "  #It follows the same steps as the flipping before\n",
        "  if train_full_df.at[row_num,'credit_risk'] == 1:\n",
        "      train_full_df.at[row_num, 'credit_risk'] = 0\n",
        "\n",
        "      accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df, predictors, target_var, 'income')\n",
        "\n",
        "      ranked_flip.append(row_num)\n",
        "      ranked_acc.append(accuracy)\n",
        "\n",
        "      ##################################################################################\n",
        "      parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "      ranked_parity.append(parity)\n",
        "\n",
        "      #train_full_df.at[index,'credit_risk'] = 1\n",
        "\n",
        "  elif train_full_df.at[row_num,'credit_risk'] == 1:\n",
        "      train_full_df.at[row_num, 'credit_risk'] = 0\n",
        "\n",
        "      accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df, predictors, target_var, 'income')\n",
        "\n",
        "      ranked_flip.append(row_num)\n",
        "      ranked_acc.append(accuracy)\n",
        "\n",
        "      ##################################################################################\n",
        "      parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "      ranked_parity.append(parity)\n",
        "\n",
        "      #train_full_df.at[index,'credit_risk'] = 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Takes the first 150 parity values and their indices and will graph those\n",
        "top_ranked_parity = ranked_parity[:]\n",
        "top_ranked_index = ranked_flip[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Place the original parity at the beginning of the list\n",
        "top_ranked_parity.insert(0,statistical_parity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4> Entropy/Uncertainty Reduction <h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create new training dataset\n",
        "train_demo_df_entropy = pd.DataFrame(x_train, columns = predictors)\n",
        "train_outcome_df_entropy = pd.DataFrame(y_train, columns = ['income'])\n",
        "train_full_df_entropy = pd.concat([train_demo_df, train_outcome_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtain predicted probabilities on the training data\n",
        "predicted_probabilities = lr.predict_proba(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Log base two would be: log = math.log(x,2)\n",
        "#Entropy equation would then be: -1*(p1((math.log(p1,2))+p2*(math.log(p2,2)))\n",
        "\n",
        "#In the predicted_probabilities: first value is 1 (which is good), second value is 2(which is bad)\n",
        "\n",
        "#Loop through the predicted_probabilities array and calculate values\n",
        "#One list will contain the index of each row in the table and the other will contain the entropy value of each row\n",
        "\n",
        "index_list = []\n",
        "entropy_list = []\n",
        "\n",
        "for index, value in enumerate(predicted_probabilities):\n",
        "  #Calculate p1 and p2\n",
        "  p1=predicted_probabilities[index,0]\n",
        "  p2=predicted_probabilities[index,1]\n",
        "  #Calculate entropy in 3 steps\n",
        "  entropy_calc1 = p1 * math.log(p1,2)\n",
        "  entropy_calc2 = p2 * math.log(p2,2)\n",
        "  entropy_final = -1 * (entropy_calc1 + entropy_calc2)\n",
        "\n",
        "  #Append the values to their respective lists\n",
        "  index_list.append(index)\n",
        "  entropy_list.append(entropy_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Rank the entropy values in decreasing order\n",
        "#Match the index list with them\n",
        "\n",
        "#Combine the two lists using zip\n",
        "combined_list = list(zip(entropy_list, index_list))\n",
        "\n",
        "#Sort the combined lust based on entropy values in descending order\n",
        "sorted_list = sorted(combined_list, key = lambda x: x[0], reverse=True)\n",
        "\n",
        "#Extract the values\n",
        "sorted_ent_list, sorted_index_list = zip(*sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_full_df_entropy['income'] = train_full_df_entropy['income'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Convert each of the tuples into lists\n",
        "ranked_indices_list = list(sorted_index_list)\n",
        "ranked_entropy_list = list(sorted_ent_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#After ranking and getting the tope 500 points I flip them based on where they are in the list\n",
        "\n",
        "#Different lists to hold each of the values\n",
        "ranked_parity = [statistical_parity]\n",
        "ranked_acc = []\n",
        "ranked_flip = []\n",
        "\n",
        "row_num = 0\n",
        "\n",
        "#Go through the training dataset and flip the points based on their entropy levels\n",
        "for row in range(len(sorted_index_list)):\n",
        "  row_num = ranked_indices_list[row]\n",
        "\n",
        "  if train_full_df_entropy.at[row_num,'income'] == 1:\n",
        "    train_full_df_entropy.at[row_num,'credit_risk'] = 0\n",
        "    accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df_entropy, predictors, target_var, 'income')\n",
        "\n",
        "    ranked_flip.append(row_num)\n",
        "    ranked_acc.append(accuracy)\n",
        "\n",
        "    parity = s_parity(test_datapoints, predictors, test_prediction, 'age', 45, 'credit_risk_12')\n",
        "    ranked_parity.append(parity)\n",
        "\n",
        "  elif train_full_df_entropy.at[row_num, 'credit_risk'] == 2:\n",
        "    train_full_df_entropy.at[row_num,'credit_risk'] = 1\n",
        "    accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df_entropy, predictors, target_var, 'credit_risk_12')\n",
        "\n",
        "    ranked_flip.append(row_num)\n",
        "    ranked_acc.append(accuracy)\n",
        "\n",
        "    parity = s_parity(test_datapoints, predictors, test_prediction, 'age', 45, 'credit_risk_12')\n",
        "    ranked_parity.append(parity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Maximum Expected Utility<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create a fresh training dataset\n",
        "train_demo_df_MEU = pd.DataFrame(x_train, columns = predictors)\n",
        "train_outcome_df_MEU = pd.DataFrame(y_train, columns = ['income'])\n",
        "train_full_df_MEU = pd.concat([train_demo_df, train_outcome_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtain predicted probabilities on the training data\n",
        "predicted_probabilities = lr.predict_proba(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Make a copy of the original training dataframe to make changes on\n",
        "copy_train_full_df = train_full_df_MEU.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Grab the original label of the DP in the copy dataframe and store in lists along with the index\n",
        "label = []\n",
        "row_index = []\n",
        "for index, row in copy_train_full_df.iterrows():\n",
        "  label.append(copy_train_full_df.at[index,'income'])\n",
        "  row_index.append(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Calculate the first part of the MEU equation\n",
        "MEU_part1_list = []\n",
        "for index, value in enumerate(label):\n",
        "  if value == 1:\n",
        "    prob1 = predicted_probabilities[index,0]\n",
        "    MEU_part1_calc = prob1 * statistical_parity\n",
        "    MEU_part1_list.append(MEU_part1_calc)\n",
        "  elif value == 2:\n",
        "    prob2 = predicted_probabilities[index,1]\n",
        "    MEU_part1_calc = prob2 * statistical_parity\n",
        "    MEU_part1_list.append(MEU_part1_calc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Iterate through the copied dataset\n",
        "#Flip each label and calculate the parity\n",
        "\n",
        "#When flipping the label make sure convert the flip before into its original value\n",
        "\n",
        "#Each list holds a different value\n",
        "\n",
        "#list_parity holds each parity value after each flip and recalculation\n",
        "MEU_part2_list_parity = []\n",
        "#list_acc holds the accuracy of each iteration after a flip\n",
        "MEU_part2_list_acc = []\n",
        "#list_flip holds the row number of the flip; starts at 0 and goes through the whole\n",
        "MEU_part2_list_flip = []\n",
        "\n",
        "#Iterating through the training dataset\n",
        "for index, row in copy_train_full_df.iterrows():\n",
        "  #If the row that I am on has the label 1 (credit_risk is the label name) then I will flip it\n",
        "  #And then run the logistic_regression function to get the accuracy, the DF that contains datapoints,\n",
        "  #And the DF that contains the predictions\n",
        "  #The two DF's are then combined in the s_parity function to calculate parity\n",
        "    if row['income'] == 1:\n",
        "      #Flip the label\n",
        "        copy_train_full_df.at[index, 'income'] = 0\n",
        "\n",
        "        #Run the logistic regression function\n",
        "        #train_full_df: training dataset\n",
        "        #predictors: IV's\n",
        "        #target_var: DV's\n",
        "        #'credit_risk12': this is the column that the labels for the test_prediction is in\n",
        "        accuracy,test_datapoints, test_prediction = logistic_regression(copy_train_full_df, predictors, target_var, 'income_01')\n",
        "\n",
        "        #list_flip: contains the row number that has just been flipped\n",
        "        #list_num: contains the accuracy value just calculated\n",
        "        MEU_part2_list_flip.append(index)\n",
        "        MEU_part2_list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        #After calculating the accuracy parity calculation is next\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income_01')\n",
        "        #list_parity: contains the parity value after the flip\n",
        "        MEU_part2_list_parity.append(parity)\n",
        "\n",
        "        #Flips the label back to its original value\n",
        "        copy_train_full_df.at[index,'income'] = 1\n",
        "\n",
        "    #Repeats all the same steps as above but this time if the original label is 2\n",
        "    elif row['income'] == 0:\n",
        "        copy_train_full_df.at[index, 'income'] = 1\n",
        "\n",
        "        accuracy,test_datapoints, test_prediction = logistic_regression(copy_train_full_df, predictors, target_var, 'income_01')\n",
        "        MEU_part2_list_flip.append(index)\n",
        "        MEU_part2_list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 45, 'income_01')\n",
        "        MEU_part2_list_parity.append(parity)\n",
        "\n",
        "        copy_train_full_df.at[index,'income'] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Second part of the calculation by taking the opposite value\n",
        "MEU_part2_list = []\n",
        "for index, value in enumerate(label):\n",
        "  if value == 1:\n",
        "    prob1 = predicted_probabilities[index,1]\n",
        "    MEU_part2_calc = prob1 * MEU_part2_list_parity[index]\n",
        "    MEU_part2_list.append(MEU_part2_calc)\n",
        "  elif value == 2:\n",
        "    prob2 = predicted_probabilities[index,0]\n",
        "    MEU_part2_calc = prob2 * MEU_part2_list_parity[index]\n",
        "    MEU_part2_list.append(MEU_part2_calc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Add the two lists together\n",
        "expected_utility_values = []\n",
        "for part1, part2 in zip(MEU_part1_list, MEU_part2_list):\n",
        "    expected_utility_values.append(part1 + part2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#After adding the two lists together I need do the following calculation:\n",
        "#EU - OG Parity = New ranked value to flip on\n",
        "new_ranked_value = []\n",
        "\n",
        "for value in expected_utility_values:\n",
        "  new_ranked_value.append(value - statistical_parity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Flip the lists so that they are in increasing order\n",
        "#Rank the MEUvalues in Increasing order\n",
        "#Match the index list with them\n",
        "\n",
        "#Combine the two lists using zip\n",
        "combined_list = list(zip(new_ranked_value, row_index))\n",
        "\n",
        "#Sort the combined lust based on EU values in increasing order\n",
        "sorted_list = sorted(combined_list, key = lambda x: abs(x[0]), reverse = True)\n",
        "\n",
        "#Extract the values\n",
        "sorted_eu_list, sorted_index_list = zip(*sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#After ranking and getting the tope 500 points I flip them based on where they are in the list\n",
        "\n",
        "#Different lists to hold each of the values\n",
        "MEU_ranked_parity = [statistical_parity]\n",
        "MEU_ranked_acc = []\n",
        "MEU_ranked_flip = []\n",
        "\n",
        "row_num = 0\n",
        "\n",
        "#Go through the training dataset and flip the points based on their entropy levels\n",
        "for row in range(len(sorted_index_list)):\n",
        "  row_num = sorted_index_list[row]\n",
        "\n",
        "  if train_full_df_MEU.at[row_num,'PINCP'] == 1:\n",
        "    train_full_df_MEU.at[row_num,'PINCP'] = 0\n",
        "    accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df_MEU, predictors, target_var, 'income')\n",
        "\n",
        "    MEU_ranked_flip.append(row_num)\n",
        "    MEU_ranked_acc.append(accuracy)\n",
        "\n",
        "    parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "    MEU_ranked_parity.append(parity)\n",
        "\n",
        "  elif train_full_df_MEU.at[row_num, 'PINCP'] == 0:\n",
        "    train_full_df_MEU.at[row_num,'PINCP'] = 1\n",
        "    accuracy,test_datapoints, test_prediction = logistic_regression(train_full_df_MEU, predictors, target_var, 'income')\n",
        "\n",
        "    MEU_ranked_flip.append(row_num)\n",
        "    MEU_ranked_acc.append(accuracy)\n",
        "\n",
        "    parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "    MEU_ranked_parity.append(parity)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Convert each of the tuples into lists\n",
        "ranked_indices_list = list(sorted_index_list)\n",
        "ranked_eu_list = list(MEU_ranked_parity)\n",
        "\n",
        "#Get the top 500 points in the list\n",
        "top_index_points = ranked_indices_list[:500]\n",
        "top_eu_list = ranked_eu_list[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Random Flipping<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create a new dataframe with the original data for the random flipping\n",
        "train_demo_df_random = pd.DataFrame(x_train, columns = predictors)\n",
        "train_outcome_df_random = pd.DataFrame(y_train, columns = ['income'])\n",
        "train_full_df_random = pd.concat([train_demo_df, train_outcome_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Randomized flipping\n",
        "#Budget of 50\n",
        "import random\n",
        "\n",
        "random_list_flip = []\n",
        "random_list_acc = []\n",
        "random_list_parity = []\n",
        "\n",
        "col_name = 'PINCP'\n",
        "budget = 332900\n",
        "\n",
        "#Loops through the budget and will randomly change rows in the dataframe based on the random library\n",
        "#After changing the row it will calculate the accuracy and parity and then change the value back to its original value\n",
        "#Follows the same flipping and calculations as before but this time it is with random rows rather than systematic flipping\n",
        "for iter in range(budget):\n",
        "    random_row = random.choice(train_full_df_random.index)\n",
        "    if train_full_df_random.at[random_row,col_name] == 1:\n",
        "        train_full_df_random.at[random_row,col_name] = 0\n",
        "\n",
        "        accuracy, test_datapoints, test_prediction = logistic_regression(train_full_df_random, predictors, target_var,'income')\n",
        "\n",
        "        random_list_flip.append(iter)\n",
        "        random_list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male','income')\n",
        "        random_list_parity.append(parity)\n",
        "\n",
        "        #train_full_df.at[random_row,col_name] = 1\n",
        "\n",
        "    elif train_full_df_random.at[random_row,col_name] == 0:\n",
        "        train_full_df_random.at[random_row,col_name] = 1\n",
        "\n",
        "        accuracy, test_datapoints, test_prediction = logistic_regression(train_full_df_random, predictors, target_var,'income')\n",
        "        random_list_flip.append(iter)\n",
        "        random_list_acc.append(accuracy)\n",
        "\n",
        "        ##################################################################################\n",
        "        parity = s_parity(test_datapoints, predictors, test_prediction, 'SEX_male', 'income')\n",
        "        random_list_parity.append(parity)\n",
        "\n",
        "        #train_full_df.at[random_row,col_name] = 2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Getting the top 150 accuracy values\n",
        "top_ranked_acc = ranked_acc[:332900]\n",
        "top_ranked_noABS_index = ranked_flip[:332900]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>Graphing Solutions<h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting the random flips dataset and the systematic flips\n",
        "#Systematic flips will have 800\n",
        "#Random will have 150\n",
        "\n",
        "#Plot the first dataset which is the ranked data list with 800 flips\n",
        "#list_parity.sort(reverse=True)\n",
        "x=range(len(list_parity))\n",
        "plt.plot(x,list_parity,label='Ranked List',color='blue')\n",
        "\n",
        "#Plot the second dataset which is the random list of 150 flips\n",
        "x2 = range(len(random_list_parity))\n",
        "plt.plot(x2,random_list_parity,label='Random List',color='red')\n",
        "\n",
        "#Entropy based flips\n",
        "x3 = range(len(ranked_parity))\n",
        "plt.plot(x3,ranked_parity,label='Uncertainty Reduction',color='black')\n",
        "\n",
        "#MEU based flips\n",
        "x4 = range(len(MEU_ranked_parity))\n",
        "plt.plot(x4,MEU_ranked_parity,label='MEU',color='brown')\n",
        "\n",
        "#Add the labels\n",
        "plt.xlabel('Flips')\n",
        "plt.ylabel('Parity')\n",
        "plt.title('Change in parity for the Random and Ranked list')\n",
        "\n",
        "#Add a legend\n",
        "plt.legend()\n",
        "\n",
        "#Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM6AJSm57uGUztTNN9k/jgx",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
